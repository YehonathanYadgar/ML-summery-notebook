{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5917ca0e-510c-4e3a-8118-4891947879c2",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784e22b2-9b79-4c36-bb2f-55bfc3b0af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea5d1e4-4eb6-4192-9f04-f12e728d334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set\n",
    "X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "y = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7434a41-5c3d-41ba-bab1-cff462f3a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, y, w, b):\n",
    "\n",
    "    # defining local vars\n",
    "    cost = 0\n",
    "    loss = 0\n",
    "    y_pred = 0\n",
    "    m = len(X)\n",
    "\n",
    "    for i in range(m):\n",
    "        # making pred with the current weights\n",
    "        y_pred = w*X[i] + b\n",
    "        loss = (y_pred - y[i])**2\n",
    "        cost += loss\n",
    "        \n",
    "    cost = cost /(2*m)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fbe9bb-4ba4-4f43-a5a7-a2d1b27e509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_calc(X,y,w,b):\n",
    "    \n",
    "    m = len(X)\n",
    "    d_w = 0\n",
    "    d_b = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        pred = w*X[i]+b\n",
    "        \n",
    "        d_w += (y[i]-pred)*X[i]\n",
    "        d_b += (y[i]-pred)\n",
    "        \n",
    "    d_w *= (-2/m) \n",
    "    d_b *= (-2/m)\n",
    "\n",
    "    return d_w, d_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf00e3d-e063-4d9c-b037-e186b807c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegression(X, y, w, b):\n",
    "\n",
    "    a = 0.01\n",
    "    d_b = 0\n",
    "    d_w = 0\n",
    "    m = len(X)\n",
    "\n",
    "    for i in range(100):\n",
    "        if i % 10 == 0:\n",
    "            print(cost(X, y, w, b))\n",
    "        \n",
    "        d_w, d_b = grad_calc(X, y, w, b)\n",
    "        \n",
    "        w += -a * d_w\n",
    "        b += -a * d_b\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fb4bb58-de25-4d1a-bfe5-a54ce7c0ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.0\n",
      "0.007804279269779495\n",
      "0.007174295455124458\n",
      "0.006595165744224451\n",
      "0.006062785045007175\n",
      "0.005573379643135202\n",
      "0.005123480449318258\n",
      "0.004709898409106042\n",
      "0.004329701897672176\n",
      "0.003980195939356661\n",
      "1.973457186117349 0.18478635542525906\n"
     ]
    }
   ],
   "source": [
    "# final run\n",
    "w, b = linearRegression(X,y,0,0)\n",
    "print(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b3e76-288f-47b4-bb80-c455a6fb3532",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fb41601-ff6a-4efe-915b-466f6630eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n",
    "y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfc021a-9dc5-4ee8-97ef-bf8d260094a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455252ae-8df8-4cdb-9acd-426933522900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cost(X,y,w,b):\n",
    "    cost = 0\n",
    "    loss = 0\n",
    "    m = len(X)\n",
    "\n",
    "    for i in range(m):\n",
    "        y_pred = sigmoid(w*X[i] + b)\n",
    "        loss = -(y[i] * np.log(y_pred) + (1 - y[i]) * np.log(1 - y_pred))\n",
    "        cost += loss\n",
    "        \n",
    "        \n",
    "    return cost /m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e783f7bf-5db5-4dc1-b960-1092f3d54c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradiant(X,y,w,b):\n",
    "    d_w = 0\n",
    "    d_b = 0\n",
    "    m = len(X)\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        z = w*X[i]+b\n",
    "        pred = sigmoid(z)\n",
    "        \n",
    "        d_w += (pred - y[i])*X[i]\n",
    "        d_b += (pred-y[i])\n",
    "        \n",
    "    d_w = d_w/m    \n",
    "    d_b = d_b/m\n",
    "    \n",
    "    return d_w,d_b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03ecb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X,y,w,b,a):\n",
    "    d_b = 0\n",
    "    d_w = 0\n",
    "    m = len(X)\n",
    "    for i in range(100):\n",
    "        if i % 10 == 0:\n",
    "            print(calc_cost(X,y,w,b))\n",
    "            \n",
    "        d_w,db = calc_gradiant(X,y,w,b)\n",
    "        w += -a*d_w\n",
    "        b += -a*d_b\n",
    "    return w,b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397424f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.770210843041627\n",
      "1.7178711594440137\n",
      "1.6661184912221336\n",
      "1.6150704821263466\n",
      "1.564870883624968\n",
      "1.5156945004030216\n",
      "1.4677521379495206\n",
      "1.4212947710642898\n",
      "1.3766156346446485\n",
      "1.3340483206439537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3102370857034073, 2.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(X,y,1,2,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d3eec9",
   "metadata": {},
   "source": [
    "# Making a class that will help us with backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5fcd4-7afd-4e50-b2ce-1f1d4dd5364b",
   "metadata": {},
   "source": [
    "A neuron is basicly a function. and in order for us to be able to optimize it with backprop we need to know the children of each node in the function, and the operation which leads to each node. and in order to do that and some more cool operation, we are going to make a class called value(which is equivalent to the autograd engine in pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34475d92-3a6b-4111-93b4-da6ddec18273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9bf08-63e4-4704-a736-4b0d3bbeab7d",
   "metadata": {},
   "source": [
    "# Making a class for building neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded9a798-2f82-4898-b235-4c4912810de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the neuron class\n",
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    # generating nin weights and 1 random bias\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "     #x = [Value(xi) for xi in x]\n",
    "    # looping through w and x and multiplying corresponding xs and ws(at the end we also add the bias)\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "      # running the linear output through an activation function\n",
    "    out = act.tanh()\n",
    "\n",
    "    return out\n",
    "\n",
    "    # making a function that returns a list of the params in a neuron. this is necessary for backprop\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75118efb-98a4-4835-bda0-54b12560f0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.9930653510988443)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [5.0,7.0]\n",
    "# defining number of features\n",
    "n = Neuron(2)\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8d03a85-41e4-4db2-ba81-a9660a020471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "  # making nouts neurons with nin features\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # looping thruge the neurons, inputing x to them, and then appending the output to a list.\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ecc1b39-fb1d-47dd-89f8-5004686a3b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=-0.9501112779653673), Value(data=-0.999127008660747), Value(data=0.40283348980863787), Value(data=0.9988921268628729)]\n"
     ]
    }
   ],
   "source": [
    "a = Layer(2,4)\n",
    "print(a(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d4f0f-45cc-4a1d-ac04-f98e2419f331",
   "metadata": {},
   "source": [
    "# Making a Neural Network(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a02815a2-239f-4ad3-bd26-e96d82d0d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "  # nin num of input features, nouts a list that specifies the number of neurons in each layer of the network.\n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "\n",
    "    # Creating a list of Layer objects, each with the current number of inputs (sz[i]) and the next number of neurons (sz[i+1])\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "\n",
    "    # looping througe each layer\n",
    "    for layer in self.layers:\n",
    "      # inputing x to the iterated layer, and assingnig its output to x .\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "  # returning all the params. \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47d7ddf2-d34a-4f40-9663-2cbabcb208be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.18633264655100948)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42f513-abe4-49ee-91fc-9e3607e9de44",
   "metadata": {},
   "source": [
    "# Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e78ba16-571a-43d4-994c-7fe2135ad93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yadga\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5f9b0-af35-4b6c-85f9-036b0812be74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
